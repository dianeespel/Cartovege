intersect_polys_corr <- st_as_sf(intersect_polys_corr) # Ensure it's an sf object
# Find dominant habitat class of FIELD samples inside each polygon of observed_MAP
print("Finding the dominant habitat class of FIELD_SAMPLES within each polygon")
# Initialize Dominant_class column as NA
intersect_polys_corr$Dominant_class <- NA
# Identify column indices for id and habitat classification
idmap = which(colnames(intersect_polys_corr) == "id")           # ID column index
ih4sample = which(colnames(intersect_polys_corr) == "Hab_L4.1") # Habitat level 4 (sample) column index
# Extract unique polygon IDs from intersected data
ID_list <- unique(intersect_polys_corr[[idmap]])
# Loop over each polygon ID
for (i in ID_list) {
# Select samples belonging to polygon i
print(paste0("Retrieving all samples present in polygon ", i, " of the map"))
polygon_of_interest <- intersect_polys_corr[intersect_polys_corr[[idmap]] == i, ]
polygon_of_interest <- st_as_sf(polygon_of_interest)
# Determine dominant habitat class among samples in polygon i
print("Finding dominant class among selected samples")
dominant_class <- get_class_dom(polygon_of_interest[[ih4sample]])
# Assign the dominant class to all samples of polygon i
intersect_polys_corr[intersect_polys_corr[[idmap]] == i, "Dominant_class"] <- dominant_class[1]
}
# Format Dominant_class habitat codes to match observed_MAP typology (insert dots)
intersect_polys_corr$Dominant_class <- sapply(intersect_polys_corr$Dominant_class, transform_chain)
# Update habitat classification columns based on dominant class
ih1map = which(colnames(intersect_polys_corr) == "Hab_L1")
ih2map = which(colnames(intersect_polys_corr) == "Hab_L2")
ih3map = which(colnames(intersect_polys_corr) == "Hab_L3")
ih4map = which(colnames(intersect_polys_corr) == "Hab_L4")
intersect_polys_corr$New_hab = intersect_polys_corr$Dominant_class
# Assign progressively detailed habitat codes to columns Hab_L1 through Hab_L4
intersect_polys_corr[, ih1map] = substr(intersect_polys_corr$New_hab, 1, 1)    # Level 1 code (first character)
intersect_polys_corr[, ih2map] = substr(intersect_polys_corr$New_hab, 1, 3)    # Level 2 code (first 3 characters)
intersect_polys_corr[, ih3map] = substr(intersect_polys_corr$New_hab, 1, 5)    # Level 3 code (first 5 characters)
intersect_polys_corr[, ih4map] = intersect_polys_corr$New_hab                   # Level 4 code (full string)
print("Corrected dataframe of intersected polygons is ready")
# Correct the original observed_MAP with updated classification from intersected polygons -------------------------------------------------------------
print("Correcting the observed_MAP using the corrected intersected polygons dataframe")
# Create a copy of observed_map for correction
observed_map_corr <- observed_map
# Identifiy id column
idmap = which(colnames(intersect_polys_corr) == "id")
# Loop over each polygon ID to update habitat classes
for (i in ID_list) {
print(paste0("Retrieving all samples present in polygon ", i, " of the map"))
# Extract corrected polygon data for polygon i
corrected_polygon <- intersect_polys_corr[intersect_polys_corr[[idmap]] == i, ]
if (nrow(corrected_polygon) > 0) {
# Update habitat classification columns in observed_map_corr with corrected values
observed_map_corr[observed_map_corr[[idmap]] == i, c("Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4")] <-
corrected_polygon[1, c("Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4")]
}
}
# Save the corrected observed map to disk as a shapefile
st_write(observed_map_corr, paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Satellite1, "_", Year1, "_EPSG32739.shp"),
driver = 'ESRI Shapefile', append = FALSE)
rm(list = ls()) # Clear all objects from the R environment to start fresh
graphics.off()   # Close all graphics devices (if any plots are open)
# Load required packages -------------------------------------------------------
library(sp)
library(sf)
library(dplyr)
library(polylabelr)
# Define global variables  ---------------------------------------------------
District='CRO' # 3-letter code for archipelago (e.g. Crozet)
Island='POS'   # 3-letter code for island (e.g. Possession)
Satellite1="Pleiades"   # Name of satellite used for multispectral imagery
Year1="2022"            # Year of multispectral imagery acquisition
# Set working directory -------------------------------------------------------------
# Base local path (customize to your local environment)
#localHOME = paste0("your_local_path/")
localHOME = paste0("D:/")
# path where to open observed map
open_ObsMap_path=paste0(localHOME,"data/vector/Observed_map/PrimaryTypo")
# path where to save your results
save_ObsMap_path=paste0(localHOME,"data/vector/Observed_map/PrimaryTypo")
# List all shapefiles matching expected filename pattern
files_corrected <- list.files(open_ObsMap_path,pattern = paste0("^Corrected_observed_map_", District, "_", Island, "_", Satellite1, "_", Year1, "_EPSG32739\\.shp$"),full.names = TRUE)
files_observed <- list.files(open_ObsMap_path, pattern = paste0("^Observed_map_", District, "_", Island, "_", Satellite1, "_", Year1, "_EPSG32739\\.shp$"),full.names = TRUE)
# If a corrected file exists, use it; otherwise, use the uncorrected version (if it exists)
if (length(files_corrected) > 0) {
FILE1 <- files_corrected[1]
} else if (length(files_observed) > 0) {
FILE1 <- files_observed[1]
}
polygons_map <- st_read(FILE1)
# Convert SpatialPolygons into sf object
polygons_map_sf=st_as_sf(polygons_map)
# Get Poles of Inaccessibility -------------------------------------------------------------
print("Computing coordinates of poles of inaccessibility for all polygons")
# Compute pole of inaccessibility
poles <- poi(polygons_map_sf)
poles <- data.table::rbindlist(poles)
# Convert poles to spatial object and filter them according to distance from polygon boundaries
distance = 5 # Define threshold distance from polygons boundaries
poles_sf <- poles %>%
filter(dist >= distance) %>%  # Filter to keep only POIs at least 5 meters from boundaries
select(1:2) %>%
rename(xcoord_m = 1, ycoord_m = 2) %>%
mutate(id = row_number()) %>%
st_as_sf(coords = c("xcoord_m", "ycoord_m"), crs = 32739)
# Intersect with original polygons to extract attributes
polesInaccess <- st_intersection(poles_sf, polygons_map_sf)
View(polesInaccess)
# Add coordinates in both EPSG:32739 and EPSG:4326
polesInaccess <- cbind(polesInaccess,
st_coordinates(polesInaccess),
st_coordinates(st_transform(polesInaccess, 4326)))
names(polesInaccess)[names(polesInaccess) == "X"] <- "xcoord_m"
names(polesInaccess)[names(polesInaccess) == "Y"] <- "ycoord_m"
names(polesInaccess)[names(polesInaccess) == "X.1"] <- "Longitude"
names(polesInaccess)[names(polesInaccess) == "Y.1"] <- "Latitude"
# Prepare final dataset
polesInaccess$Date = "2022"
polesInaccess$Source = "PHOTO-INTERPRETATION"
polesInaccess = polesInaccess[, c("id", "Date", "Source", "xcoord_m", "ycoord_m", "Longitude", "Latitude", "Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4", "geometry")]
View(polesInaccess)
# Save results
st_write(polesInaccess, paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Poles_of_inaccessibility_EPSG32739.shp"), driver = 'ESRI Shapefile', append = FALSE)
write.table(polesInaccess, file = paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Poles_of_inaccessibility_EPSG32739.csv"), sep = ";", dec = ".", row.names = FALSE)
print("Computing coordinates of internal surface points for all polygons")
# Compute internal points (centroids)
ptOnSurface <- polygons_map_sf %>%
st_point_on_surface() %>%
mutate(id = max(polesInaccess$id) + row_number())
# Add coordinates in both EPSG:32739 and EPSG:4326
ptOnSurface <- cbind(ptOnSurface,
st_coordinates(ptOnSurface),
st_coordinates(st_transform(ptOnSurface, 4326)))
names(ptOnSurface)[names(ptOnSurface) == "X"] <- "xcoord_m"
names(ptOnSurface)[names(ptOnSurface) == "Y"] <- "ycoord_m"
names(ptOnSurface)[names(ptOnSurface) == "X.1"] <- "Longitude"
names(ptOnSurface)[names(ptOnSurface) == "Y.1"] <- "Latitude"
# Prepare final dataset
ptOnSurface$Date = "2022"
ptOnSurface$Source = "PHOTO-INTERPRETATION"
ptOnSurface = ptOnSurface[, c("id", "Date", "Source", "xcoord_m", "ycoord_m", "Longitude", "Latitude", "Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4", "geometry")]
# Save results
st_write(ptOnSurface, paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Pts_on_surface_EPSG32739.shp"), driver = 'ESRI Shapefile', append = FALSE)
write.table(ptOnSurface, file = paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Pts_on_surface_EPSG32739.csv"), sep = ";", dec = ".", row.names = FALSE)
# Computing coordinates from random points
print("Computing coordinates for additional random points inside large polygons")
# Identify large polygons
Area_min=2000
big_polygons_map_sf=subset(polygons_map_sf,polygons_map_sf$Surface>=Area_min)
print(paste0("Il y a ",nrow(big_polygons_map_sf)," polygones qui font plus de ", Area_min," m2"))
# Fix random seed for reproducibility
set.seed(3024)
# Set parameters
n_random <- 8  # Number of random points per polygon
distance <- 12  # Minimum distance (m) between random points
# List to store generated points
points_within_polygons <- list()
# Generate random points within each large polygon
for (p in 1:nrow(big_polygons_map_sf)) {
# Select the current polygon
each_polygon <- big_polygons_map_sf[p, ]
# Plot the polygon before generating points
plot(each_polygon$geometry, col = "lightblue", main = paste("Polygon ID:", each_polygon$id))
# Generate random points within the polygon
random_points <- st_sample(each_polygon, size = n_random, exact = TRUE)
# Check if any points were generated
if (length(random_points) > 0) {
# Plot the generated random points inside the polygon
plot(random_points, col = "red", add = TRUE, pch = 20) # Red points on top of the polygon
# Create an sf object with random points and attributes
random_points_sf <- st_sf(
id = rep(each_polygon$id, each = length(random_points)),
Nom_site = rep(each_polygon$Nom_site, each = length(random_points)),
Surface = rep(each_polygon$Surface, each = length(random_points)),
Hab_L1 = rep(each_polygon$Hab_L1, each = length(random_points)),
Hab_L2 = rep(each_polygon$Hab_L2, each = length(random_points)),
Hab_L3 = rep(each_polygon$Hab_L3, each = length(random_points)),
Hab_L4 = rep(each_polygon$Hab_L4, each = length(random_points)),
geometry = random_points # Set geometry column
)
# Compute coordinates in EPSG:32739 (meters)
coords_32739 <- st_coordinates(random_points_sf)
colnames(coords_32739) <- c("xcoord_m", "ycoord_m")
# Transform to EPSG:4326 (Latitude & Longitude)
random_points_sf <- st_transform(random_points_sf, crs = 4326)
coords_4326 <- st_coordinates(random_points_sf)
colnames(coords_4326) <- c("Longitude", "Latitude")
# Append coordinate columns to the points dataset
random_points_sf <- cbind(random_points_sf, coords_32739, coords_4326)
# Store the results in the list
points_within_polygons[[p]] <- random_points_sf
}
}
# Merge all generated points into a single sf object
RandomPts <- do.call(rbind, points_within_polygons)
# Assign a new unique ID to each generated point
RandomPts$id <- (max(ptOnSurface$id, na.rm = TRUE) + 1):(max(ptOnSurface$id, na.rm = TRUE) + nrow(RandomPts))
#Transform to EPSG:32739 (xcoord_m nd ycoord_m)
RandomPts <- st_transform(RandomPts, crs = 32739)
# Prepare final file
RandomPts$Date = "2022"
RandomPts$Source = "PHOTO-INTERPRETATION"
RandomPts=RandomPts[,c("id","Date","Source","xcoord_m","ycoord_m","Longitude","Latitude","Hab_L1","Hab_L2","Hab_L3","Hab_L4","geometry")]
# Save as Shapefile
st_write(RandomPts, paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Random_points_EPSG32739.shp"),driver = 'ESRI Shapefile', append = FALSE)
write.table(RandomPts,file=paste0(save_ObsMap_path, "/", "Corrected_observed_map_", District, "_", Island, "_", Year1, "_Random_points_EPSG32739.csv"),sep = ";", dec = ".", row.names = FALSE)
rm(list = ls()) # Clear all objects from the R environment to start fresh
graphics.off()   # Close all graphics devices (if any plots are open)
# Load required packages -------------------------------------------------------
library(sp)       # For spatial data structures (legacy)
library(sf)       # For modern simple features spatial data handling
library(terra)    # For raster and vector data manipulation (newer alternative to raster)
library(dplyr)    # For bind_rows() function
# Create fucntions  ---------------------------------------------------
# Function to build a square polygon from bounding box coordinates
create_quadrat <- function(xmin, xmax, ymin, ymax) {
st_polygon(list(matrix(c(
xmin, ymax,  # Top-left (NW)
xmax, ymax,  # Top-right (NE)
xmax, ymin,  # Bottom-right (SE)
xmin, ymin,  # Bottom-left (SW)
xmin, ymax   # Close polygon (back to NW)
), ncol = 2, byrow = TRUE)))
}
# Define global variables  ---------------------------------------------------
District='CRO' # 3-letter code for archipelago (e.g. Crozet)
Island='POS'   # 3-letter code for island (e.g. Possession)
Year1="2022"            # Year of multispectral imagery acquisition
# Base local path (customize to your local environment)
#localHOME = paste0("your_local_path/")
localHOME = paste0("D:/")
# Path to open input  photo-interpreted points
open_ObsMap_path=paste0(localHOME,"data/vector/Observed_map/PrimaryTypo")
# Path to save photo-interpreted plots
save_plots_path=paste0(localHOME,"data/vector/Plots/PrimaryTypo")
# Read shapefiles ----------------------------------------------------
# Define full paths to the shapefiles to be loaded
FILE1 <- paste0(open_ObsMap_path, paste0("/","Corrected_observed_map_", District, "_", Island, "_", Year1, "_Poles_of_inaccessibility_EPSG32739.shp"))
FILE2 <-  paste0(open_ObsMap_path, paste0("/","Corrected_observed_map_", District, "_", Island, "_", Year1, "_Pts_on_surface_EPSG32739.shp"))
FILE3 <-  paste0(open_ObsMap_path, paste0("/","Corrected_observed_map_", District, "_", Island, "_", Year1, "_Random_points_EPSG32739.shp"))
# Load the shapefiles
PolesInaccess <- st_read(FILE1)
PtOnSurface <- st_read(FILE2)
RandomPts <- st_read(FILE3)
# Identify common columns shared between all three layers
common_columns <- Reduce(intersect, list(names(PolesInaccess ), names(PtOnSurface), names(RandomPts)))
cat("Common columns:\n")
print(common_columns)
# Subset each layer to retain only the common columns
PolesInaccess  <- PolesInaccess [, common_columns]
PtOnSurface <- PtOnSurface[, common_columns]
RandomPts <- RandomPts[, common_columns]
# Merge all point layers into one
all_points <- bind_rows(PolesInaccess , PtOnSurface, RandomPts)
# Check for duplicate IDs and correct if needed
if (any(duplicated(all_points$id))) {
warning("Duplicate IDs detected — correcting them.")
print(unique(all_points$id[duplicated(all_points$id)]))
all_points$id <- as.character(seq_len(nrow(all_points))) # Replace with unique IDs
} else {
print("No duplicate IDs.")
}
any(duplicated(all_points$id))
length_quadrat <- 4        # Length of square side in meters
radius <- length_quadrat / 2  # Half-length for buffer calculation
# Coordinates must be in meters (EPSG:32739 UTM Zone 39S)
all_points <- all_points %>%
mutate(
xcoord_m = st_coordinates(.)[,1],   # Extract X (easting)
ycoord_m = st_coordinates(.)[,2],   # Extract Y (northing)
xMinus = xcoord_m - radius,         # Lower X bound
xPlus  = xcoord_m + radius,         # Upper X bound
yMinus = ycoord_m - radius,         # Lower Y bound
yPlus  = ycoord_m + radius,         # Upper Y bound
Surface = (length_quadrat^2)        # Quadrat area
)
# Add metadata columns
all_points$Source <- "PHOTO-INTERPRETATION"
all_points$Date <- "2022"
# Generate list of quadrats as polygons
quadrat_list <- mapply(
create_quadrat,
all_points$xMinus, all_points$xPlus,
all_points$yMinus, all_points$yPlus,
SIMPLIFY = FALSE)
# Create an sf object for the quadrats with selected attribute columns
Spdf_field_polys <- st_sf(
all_points %>%
select(id, Longitude, Latitude, xcoord_m, ycoord_m, Date, Source, Surface, Hab_L1, Hab_L2, Hab_L3, Hab_L4),
geometry = st_sfc(quadrat_list, crs = 32739))
cat("Filtering overlapping quadrats...\n")
print("If overlap occurs, keep the first quadrat and remove the intersecting ones.")
keep_rows <- rep(TRUE, nrow(Spdf_field_polys)) # Start by keeping all rows
# Loop to detect overlaps and flag the later duplicates
for (i in seq_len(nrow(Spdf_field_polys))) {
if (!keep_rows[i]) next  # Skip if already excluded
for (j in seq((i+1), nrow(Spdf_field_polys))) {
if (!keep_rows[j]) next
if (st_intersects(Spdf_field_polys[i, ], Spdf_field_polys[j, ], sparse = FALSE)[1,1]) {
cat(sprintf("Entity %d overlaps with %d — removing %d\n", j, i, j))
keep_rows[j] <- FALSE
}
}
}
# Créer la matrice d’intersections
inter_mat <- st_intersects(Spdf_field_polys, sparse = FALSE)
# Créer un graphe des polygones qui se touchent
g <- graph_from_adjacency_matrix(inter_mat, mode = "undirected", diag = FALSE)
library(sf)
library(igraph)
install.packages("igraph")
library(sf)
library(igraph)
# Créer un graphe des polygones qui se touchent
g <- graph_from_adjacency_matrix(inter_mat, mode = "undirected", diag = FALSE)
# Identifier les composantes connexes (groupes d'intersections)
components <- components(g)
# Garder un seul polygone par groupe (le premier)
keep_ids <- tapply(seq_along(components$membership),
components$membership,
function(x) x[1])
# Write final quadrat polygons to a shapefile
st_write(Spdf_field_polys, paste0(save_plots_path, "/Tests_EPSG32739.shp"), driver = "ESRI Shapefile", append = FALSE)
# Matrice logique indiquant quelles entités se chevauchent (pas juste s'intersectent)
overlap_matrix <- st_overlaps(Spdf_field_polys), sparse = FALSE)
# Matrice logique indiquant quelles entités se chevauchent (pas juste s'intersectent)
overlap_matrix <- st_overlaps(Spdf_field_polys, sparse = FALSE)
# Trouver les indices des entités qui se chevauchent avec au moins une autre
overlapping_entities <- which(rowSums(overlap_matrix) > 0)
# Extraire ces polygones chevauchants
overlapping_polygons <- polygons_map_sf[overlapping_entities, ]
# Extraire ces polygones chevauchants
overlapping_polygons <- Spdf_field_polys[overlapping_entities, ]
View(overlapping_polygons)
# Nombre d'entités
n <- nrow(Spdf_field_polys)
# Vecteur pour marquer les polygones à garder (TRUE = garder)
keep <- rep(TRUE, n)
for (i in 1:n) {
if (!keep[i]) next  # Si déjà exclu, on skip
# Trouver les polygones qui chevauchent i (hors i lui-même)
overlapping <- which(overlap_matrix[i, ] & (1:n != i))
# Marquer ces chevauchements comme à supprimer (keep = FALSE)
keep[overlapping] <- FALSE
}
# Filtrer les polygones à garder
filtered_polys <- Spdf_field_polys[keep, ]
11089-9996
# Filter to retain only non-overlapping quadrats
Allpolys <- Spdf_field_polys[keep_rows, ]
# Filter to retain only non-overlapping quadrats
Allpolys <- Spdf_field_polys[keep, ]
View(Allpolys)
# Reorder columns for final export
Allpolys <- Allpolys[, c("id", "xcoord_m", "ycoord_m", "Longitude", "Latitude", "Date", "Source", "Surface", "Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4", "geometry")]
# Rename columns to match export conventions
names(Allpolys) <- c("N_obs", "xcoord_m", "ycoord_m", "Longitude", "Latitude", "Date", "Source", "Surface", "Hab_L1", "Hab_L2", "Hab_L3", "Hab_L4", "geometry")
# Write final quadrat polygons to a shapefile
st_write(Allpolys, paste0(save_plots_path, "/Quadrats_", District, "_", Island, "_PHOTO-INTERPRETED_Polygons_EPSG32739.shp"), driver = "ESRI Shapefile", append = FALSE)
# Generate centroids from the quadrats
centroids <- st_centroid(Allpolys)
# Ensure correct geometry structure for output
centroids <- st_as_sf(centroids, coords = c("xcoord_m", "ycoord_m"), crs = 32739)
# Write centroid shapefile
st_write(centroids, paste0(save_plots_path, "/Quadrats_", District, "_", Island, "_PHOTO-INTERPRETED_Centroids_EPSG32739.shp"), driver = "ESRI Shapefile", append = FALSE)
rm(list = ls()) # Clear all objects from the R environment to start fresh
graphics.off()   # Close all graphics devices (if any plots are open)
# Required packages -------------------------------------------------------
library(sf)     # For handling spatial vector data (modern replacement for 'rgdal')
# Define global variables  ---------------------------------------------------
District='CRO' # 3-letter code for archipelago (e.g. Crozet)
Island='POS'   # 3-letter code for island (e.g. Possession)
# Set working directory -------------------------------------------------------------
# Base local path (customize to your local environment)
#localHOME = paste0("your_local_path/")
localHOME = paste0("D:/")
# Path to open input multisources plots
open_plots_path=paste0(localHOME,"data/vector/Plots/PrimaryTypo")
# Path to save all learning plots
save_plots_path=paste0(localHOME,"data/vector/Plots/PrimaryTypo")
# Define the file paths for the three possible shapefile sources:
# 1. FIELD samples (mandatory)
# 2. HFI samples (optional)
# 3. Photointerpreted plots (optional)
FILE1 <- paste0(open_plots_path, "/Quadrats_", District, "_", Island, "_ALL_FIELD_SAMPLES_Polygons_corrected_EPSG32739.shp")
FILE2 <- paste0(open_plots_path, "/Quadrats_", District, "_", Island, "_ALL_HFI_SAMPLES_Polygons_corrected_EPSG32739.shp")
FILE3 <- paste0(open_plots_path, "/Quadrats_", District, "_", Island, "_PHOTO-INTERPRETED_Polygons_EPSG32739.shp")
# Load the FIELD shapefile (mandatory)
FIELD_true_plots <- st_read(FILE1)
# If HFI file exists, load it
if (file.exists(FILE2)) {
assign("HFI_true_plots", st_read(FILE2))
}
# If photointerpreted file exists, load it
if (file.exists(FILE3)) {
assign("photointerpreted_plots", st_read(FILE3))
}
# Merge plots and filter overlapping plots -------------------------------------------------------------
# Start with FIELD plots (always present)
all_plots<- FIELD_true_plots
# If photointerpreted plots exist, append them to the dataset
if (exists("photointerpreted_plots")) {
all_plots<- rbind(all_plots, photointerpreted_plots)
}
# If HFI plots exist append them to the dataset
if (exists("HFI_true_plots")) {
all_plots<- rbind(all_plots, HFI_true_plots)
}
# At this stage, all_plots contains FIELD plots and other sources if they exist
# Define a priority order for conflict resolution: lower number = higher priority
priority_order <- c("FIELD" = 1, "PHOTO-INTERPRETATION" = 2, "HFI" = 3) # assuming HFI correspond to historical values
all_plots$priority <- priority_order[ all_plots$Source ] # Create a priority column based on the Source column
# Compute all intersecting pairs (returns a list of indices)
intersections_list <- st_intersects(all_plots, sparse = TRUE)
# Create a data frame of intersecting pairs i-j with i < j
pairs <- do.call(rbind, lapply(seq_along(intersections_list), function(i) {
j_vec <- intersections_list[[i]]
j_vec <- j_vec[j_vec > i]  # keep only pairs where j > i to avoid duplicates
if(length(j_vec) == 0) return(NULL)
data.frame(i = i, j = j_vec)
}))
# Vector to keep track of plots to retain
keep <- rep(TRUE, nrow(all_plots))
# Resolve overlaps by priority
for (k in seq_len(nrow(pairs))) {
if (!keep[pairs$i[k]] || !keep[pairs$j[k]]) next
i_prio <- all_plots$priority[pairs$i[k]]
j_prio <- all_plots$priority[pairs$j[k]]
if (i_prio <= j_prio) {
keep[pairs$j[k]] <- FALSE
} else {
keep[pairs$i[k]] <- FALSE
}
}
filtered_plots <- all_plots[keep, ]
View(filtered_plots)
# Compute all intersecting pairs (list of neighbors for each polygon)
intersections_list <- st_intersects(all_plots, sparse = TRUE)
# Create a data frame with pairs (i,j) where i < j to avoid duplicates
pairs <- do.call(rbind, lapply(seq_along(intersections_list), function(i) {
js <- intersections_list[[i]]
js <- js[js > i]  # only keep pairs where j > i
if (length(js) == 0) return(NULL)
data.frame(i = i, j = js)
}))
# Initialize vector to keep track of polygons to retain
keep <- rep(TRUE, nrow(all_plots))
# Loop over intersecting pairs to resolve overlaps by priority
for (k in seq_len(nrow(pairs))) {
i <- pairs$i[k]
j <- pairs$j[k]
# Skip if either polygon already excluded
if (!keep[i] || !keep[j]) next
# Compare priorities (lower number = higher priority)
if (all_plots$priority[i] <= all_plots$priority[j]) {
keep[j] <- FALSE
} else {
keep[i] <- FALSE
}
}
# Filter the polygons to keep only the highest priority non-overlapping ones
filtered_plots <- all_plots[keep, ]
View(all_plots)
View(all_plots)
# Start with FIELD plots (always present)
all_plots<- FIELD_true_plots
# If photointerpreted plots exist, append them to the dataset
if (exists("photointerpreted_plots")) {
all_plots<- rbind(all_plots, photointerpreted_plots)
}
# If HFI plots exist append them to the dataset
if (exists("HFI_true_plots")) {
all_plots<- rbind(all_plots, HFI_true_plots)
}
# Define a priority order for conflict resolution: lower number = higher priority
priority_order <- c("FIELD" = 1, "PHOTO-INTERPRETATION" = 2, "HFI" = 3) # assuming HFI correspond to historical values
all_plots$priority <- priority_order[ all_plots$Source ] # Create a priority column based on the Source column
# Compute all intersecting pairs (list of neighbors for each polygon)
intersections_list <- st_intersects(all_plots, sparse = TRUE)
# Create a data frame with pairs (i,j) where i < j to avoid duplicates
pairs <- do.call(rbind, lapply(seq_along(intersections_list), function(i) {
js <- intersections_list[[i]]
js <- js[js > i]  # only keep pairs where j > i
if (length(js) == 0) return(NULL)
data.frame(i = i, j = js)
}))
View(pairs)
# Initialize vector to keep track of polygons to retain
keep <- rep(TRUE, nrow(all_plots))
# Loop over intersecting pairs to resolve overlaps by priority
for (k in seq_len(nrow(pairs))) {
i <- pairs$i[k]
j <- pairs$j[k]
# Skip if either polygon already excluded
if (!keep[i] || !keep[j]) next
# Compare priorities (lower number = higher priority)
if (all_plots$priority[i] <= all_plots$priority[j]) {
keep[j] <- FALSE
} else {
keep[i] <- FALSE
}
}
# Filter the polygons to keep only the highest priority non-overlapping ones
filtered_plots <- all_plots[keep, ]
# Print final retained plots to the console
print(filtered_plots)
View(filtered_plots)
# Rename N_obs column into ID column
names(filtered_plots)[names(filtered_plots) == "N_obs"] <- "ID"
View(filtered_plots)
# Save the filtered plots to a shapefile
st_write(filtered_plots, paste0(save_plots_path, "/Quadrats_", District, "_", Island, "_ALL_SOURCES_Polygons_EPSG32739.shp"), driver = "ESRI Shapefile", append = FALSE)
# Extract centroids from filtered plots and save as a new shapefile and as a csv file for tabular use
filtered_plots_sf <- st_as_sf(filtered_plots) # Ensure it's an sf object
centroids <- st_centroid(filtered_plots_sf) # Compute centroids
st_write(centroids, paste0(save_plots_path, "/Quadrats_", District, "_", Island, "_ALL_SOURCES_Centroids_EPSG32739.shp"),driver = "ESRI Shapefile", append = FALSE)
